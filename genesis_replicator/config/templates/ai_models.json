{
    "gpt4": {
        "provider": "openai",
        "model": "gpt-4",
        "max_tokens": 8192,
        "temperature": 0.7,
        "top_p": 1.0,
        "frequency_penalty": 0.0,
        "presence_penalty": 0.0,
        "rate_limits": {
            "requests_per_minute": 200,
            "tokens_per_minute": 40000
        },
        "retry_config": {
            "max_retries": 3,
            "initial_delay": 1,
            "max_delay": 10,
            "backoff_factor": 2
        },
        "fallback_models": ["gpt-3.5-turbo", "claude-3-opus"]
    },
    "claude3": {
        "provider": "anthropic",
        "model": "claude-3-opus",
        "max_tokens": 100000,
        "temperature": 0.7,
        "top_p": 1.0,
        "rate_limits": {
            "requests_per_minute": 150,
            "tokens_per_minute": 50000
        },
        "retry_config": {
            "max_retries": 3,
            "initial_delay": 1,
            "max_delay": 10,
            "backoff_factor": 2
        },
        "fallback_models": ["claude-3-sonnet", "gpt-4"]
    },
    "llama2": {
        "provider": "meta",
        "model": "llama-2-70b",
        "max_tokens": 4096,
        "temperature": 0.7,
        "top_p": 1.0,
        "rate_limits": {
            "requests_per_minute": 100,
            "tokens_per_minute": 30000
        },
        "retry_config": {
            "max_retries": 3,
            "initial_delay": 1,
            "max_delay": 10,
            "backoff_factor": 2
        },
        "fallback_models": ["llama-2-13b", "gpt-3.5-turbo"]
    },
    "mistral": {
        "provider": "mistral",
        "model": "mistral-large",
        "max_tokens": 8192,
        "temperature": 0.7,
        "top_p": 1.0,
        "rate_limits": {
            "requests_per_minute": 120,
            "tokens_per_minute": 35000
        },
        "retry_config": {
            "max_retries": 3,
            "initial_delay": 1,
            "max_delay": 10,
            "backoff_factor": 2
        },
        "fallback_models": ["mistral-medium", "gpt-3.5-turbo"]
    },
    "gemini": {
        "provider": "google",
        "model": "gemini-pro",
        "max_tokens": 8192,
        "temperature": 0.7,
        "top_p": 1.0,
        "rate_limits": {
            "requests_per_minute": 150,
            "tokens_per_minute": 45000
        },
        "retry_config": {
            "max_retries": 3,
            "initial_delay": 1,
            "max_delay": 10,
            "backoff_factor": 2
        },
        "fallback_models": ["gemini-1.0-pro", "gpt-3.5-turbo"]
    }
}
